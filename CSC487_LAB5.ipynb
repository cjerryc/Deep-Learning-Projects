{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"rgCK70fgf0rN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708540371452,"user_tz":480,"elapsed":296646,"user":{"displayName":"Jerry Chang","userId":"16741169191998840800"}},"outputId":"931ebed9-8eb8-46e2-ad49-68174a1528f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["65 unique characters\n","Epoch 1/20\n","172/172 [==============================] - 14s 58ms/step - loss: 2.7364\n","Epoch 2/20\n","172/172 [==============================] - 11s 57ms/step - loss: 2.0004\n","Epoch 3/20\n","172/172 [==============================] - 12s 59ms/step - loss: 1.7207\n","Epoch 4/20\n","172/172 [==============================] - 12s 60ms/step - loss: 1.5572\n","Epoch 5/20\n","172/172 [==============================] - 12s 61ms/step - loss: 1.4556\n","Epoch 6/20\n","172/172 [==============================] - 12s 58ms/step - loss: 1.3855\n","Epoch 7/20\n","172/172 [==============================] - 12s 59ms/step - loss: 1.3314\n","Epoch 8/20\n","172/172 [==============================] - 12s 60ms/step - loss: 1.2864\n","Epoch 9/20\n","172/172 [==============================] - 13s 58ms/step - loss: 1.2442\n","Epoch 10/20\n","172/172 [==============================] - 12s 59ms/step - loss: 1.2040\n","Epoch 11/20\n","172/172 [==============================] - 12s 61ms/step - loss: 1.1635\n","Epoch 12/20\n","172/172 [==============================] - 12s 61ms/step - loss: 1.1226\n","Epoch 13/20\n","172/172 [==============================] - 12s 59ms/step - loss: 1.0787\n","Epoch 14/20\n","172/172 [==============================] - 12s 59ms/step - loss: 1.0324\n","Epoch 15/20\n","172/172 [==============================] - 12s 59ms/step - loss: 0.9833\n","Epoch 16/20\n","172/172 [==============================] - 12s 59ms/step - loss: 0.9321\n","Epoch 17/20\n","172/172 [==============================] - 12s 60ms/step - loss: 0.8795\n","Epoch 18/20\n","172/172 [==============================] - 12s 60ms/step - loss: 0.8268\n","Epoch 19/20\n","172/172 [==============================] - 12s 59ms/step - loss: 0.7751\n","Epoch 20/20\n","172/172 [==============================] - 12s 58ms/step - loss: 0.7268\n","To be or not to be conquer'd,\n","For the rebute may be a much delivered.\n","Sirs, I will know, not for thy rangerous,\n","But, why reports the queen then our privites in me\n","wounded here, Throng to the throw! deathle again;\n","Not yet that terms the spirit, mine! they so she's made\n","A brace of deeps into such merchand.\n","I tell you what we waste of this damnable.\n","How is it!--he could not reason what has made me found?\n","Is not the boy will strike her hence?\n","\n","LADY GREY:\n","'Tis good Calbs not for the none of his proud sun,\n","Jeicilbers will I drew my face.\n","And when the truth that runs barce to suppose\n","Upon the serps-o' the state, whose fathers be\n","deliver'd truly, keep thee of thy blessing.\n","\n","HENRY BOLINGBROKE:\n","Pardon me, Grumio, mark me, as you did, would here do see his presence?\n","Now, brother, I cry to thy life, I would\n","But leave in banish'd from the kennels three loves.\n","\n","QUEEN MARGARET:\n","Gramercies eloquent difl'd;\n","But that they have frown my dumpess by the sword\n","Were rather valiant golden days,\n","For ever I stand too meet again. \n","\n","________________________________________________________________________________\n","\n","Run time: 3.260874032974243\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import time\n","\n","path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","# The unique characters in the file\n","vocab = sorted(set(text))\n","print(f'{len(vocab)} unique characters')\n","\n","example_texts = ['abcdefg', 'xyz']\n","\n","chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n","# Returns Index from set of characters in text\n","ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n","ids = ids_from_chars(chars)\n","# Returns Characters from set of text Indices\n","chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n","chars = chars_from_ids(ids)\n","\n","# Given indices, rejoin the characters back together into regular text\n","def text_from_ids(ids):\n","  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n","\n","# Split all characters in text dataset into individual ASCII unicode and store in a tensor\n","all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n","ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n","# Segment text characters into batches of 100\n","seq_length = 100\n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep\n","def split_input_target(sequence):\n","  input_text = sequence[:-1]\n","  target_text = sequence[1:]\n","  return input_text, target_text\n","# Example:\n","split_input_target(list(\"Tensorflow\"))\n","dataset = sequences.map(split_input_target)\n","\n","# Batch size\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","# Length of the vocabulary\n","vocab_size = len(ids_from_chars.get_vocabulary())\n","# The embedding dimension\n","embedding_dim = 256\n","# Number of RNN units\n","rnn_units = 1024\n","\n","# Functions for Initializing and Calling a Model\n","class MyModel(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self)\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(rnn_units,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    x = self.embedding(x, training=training)\n","    if states is None:\n","      states = self.gru.get_initial_state(x)\n","    x, states = self.gru(x, initial_state=states, training=training)\n","    x = self.dense(x, training=training)\n","\n","    if return_state:\n","      return x, states\n","    else:\n","      return x\n","\n","# Create a Model\n","model = MyModel(\n","    vocab_size=vocab_size,\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units)\n","\n","for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n","\n","# Train the model\n","loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n","example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n","model.compile(optimizer='adam', loss=loss)\n","\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)\n","\n","EPOCHS = 20\n","\n","# Fit the model\n","history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n","\n","# Generate Text with a Single-Step Prediction approach, keep track of the model's internal state as you execute it.\n","class OneStep(tf.keras.Model):\n","  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.chars_from_ids = chars_from_ids\n","    self.ids_from_chars = ids_from_chars\n","\n","    # Create a mask to prevent \"[UNK]\" from being generated.\n","    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        # Put a -inf at each bad index.\n","        values=[-float('inf')]*len(skip_ids),\n","        indices=skip_ids,\n","        # Match the shape to the vocabulary\n","        dense_shape=[len(ids_from_chars.get_vocabulary())])\n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs.\n","    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_chars(input_chars).to_tensor()\n","    predicted_logits, states = self.model(inputs=input_ids, states=states,\n","                                          return_state=True)\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","    predicted_logits = predicted_logits + self.prediction_mask\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","    predicted_chars = self.chars_from_ids(predicted_ids)\n","    return predicted_chars, states\n","\n","one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n","\n","# Run it in a loop to generate some text\n","start = time.time()\n","states = None\n","next_char = tf.constant(['To be or not to be'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNAJBRf2cyGCfXkj2SQkbHy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}