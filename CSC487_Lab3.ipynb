{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2epyuqQSjeideesAOF2BZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VeBR8FwRbfZA","executionInfo":{"status":"ok","timestamp":1706413266428,"user_tz":480,"elapsed":180,"user":{"displayName":"Jerry Chang","userId":"16741169191998840800"}},"outputId":"d2678ce5-506a-4e79-8b66-671f90cea9ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.08308768 0.2343672  0.26379581]\n","5.064717861811059\n","[0.08942297 0.23010172 0.23712781]\n","4.761718353081214\n","[0.06867058 0.16785013 0.25436296]\n","4.290801550066071\n","\n"," ####################################### Different Learning Rates ############################################################################## \n","\n","Gradient Descent for Learning Rate: 0.005 produces the resulting Weight: [0.07059087 0.22680162 0.27245693]\n","Output of first sample: 5.063176428344221\n","Gradient Descent for Learning Rate: 0.006 produces the resulting Weight: [0.07318702 0.22887243 0.27041922]\n","Output of first sample: 5.066103514092308\n","Gradient Descent for Learning Rate: 0.007 produces the resulting Weight: [0.07574266 0.23066254 0.26859556]\n","Output of first sample: 5.068323675470176\n","Gradient Descent for Learning Rate: 0.008 produces the resulting Weight: [0.07825763 0.23220412 0.26696256]\n","Output of first sample: 5.0699244546592235\n","Gradient Descent for Learning Rate: 0.009000000000000001 produces the resulting Weight: [0.08072946 0.23351721 0.26548713]\n","Output of first sample: 5.070772863761281\n","Gradient Descent for Learning Rate: 0.01 produces the resulting Weight: [0.08308768 0.2343672  0.26379581]\n","Output of first sample: 5.064717861811059\n","Gradient Descent for Learning Rate: 0.011 produces the resulting Weight: [0.08454045 0.23184891 0.25788725]\n","Output of first sample: 4.981902015425214\n","Gradient Descent for Learning Rate: 0.012 produces the resulting Weight: [0.08004708 0.20734205 0.22238396]\n","Output of first sample: 4.377307188133534\n","Gradient Descent for Learning Rate: 0.013000000000000001 produces the resulting Weight: [0.0473175  0.07843527 0.04512161]\n","Output of first sample: 1.2828863280458438\n","Gradient Descent for Learning Rate: 0.014000000000000002 produces the resulting Weight: [-0.09045303 -0.43888176 -0.66034678]\n","Output of first sample: -11.08273846034786\n","\n","\n","Mini_Batch Gradient Descent for Learning Rate: 0.005 produces the resulting Weight: [0.07040374 0.22406437 0.26194251]\n","Output of first sample: 4.930472613809863\n","Mini_Batch Gradient Descent for Learning Rate: 0.006 produces the resulting Weight: [0.07374748 0.22583686 0.257075  ]\n","Output of first sample: 4.902866008071356\n","Mini_Batch Gradient Descent for Learning Rate: 0.007 produces the resulting Weight: [0.07731375 0.2272987  0.25214823]\n","Output of first sample: 4.871783103982826\n","Mini_Batch Gradient Descent for Learning Rate: 0.008 produces the resulting Weight: [0.08111323 0.22848771 0.24718593]\n","Output of first sample: 4.837849677449912\n","Mini_Batch Gradient Descent for Learning Rate: 0.009000000000000001 produces the resulting Weight: [0.0851493  0.22941842 0.24218209]\n","Output of first sample: 4.801154362064871\n","Mini_Batch Gradient Descent for Learning Rate: 0.01 produces the resulting Weight: [0.08942297 0.23010172 0.23712781]\n","Output of first sample: 4.761718353081214\n","Mini_Batch Gradient Descent for Learning Rate: 0.011 produces the resulting Weight: [0.09393345 0.23054798 0.23201567]\n","Output of first sample: 4.719569988625421\n","Mini_Batch Gradient Descent for Learning Rate: 0.012 produces the resulting Weight: [0.09867802 0.23076749 0.22684047]\n","Output of first sample: 4.674757636915043\n","Mini_Batch Gradient Descent for Learning Rate: 0.013000000000000001 produces the resulting Weight: [0.10365176 0.23077063 0.2215996 ]\n","Output of first sample: 4.627354051261985\n","Mini_Batch Gradient Descent for Learning Rate: 0.014000000000000002 produces the resulting Weight: [0.10884742 0.23056793 0.21629329]\n","Output of first sample: 4.577459640898558\n","\n","\n","Stochastic Gradient Descent for Learning Rate: 0.005 produces the resulting Weight: [0.02438858 0.15449488 0.15156963]\n","Output of first sample: 3.0850336981677993\n","Stochastic Gradient Descent for Learning Rate: 0.006 produces the resulting Weight: [0.03284133 0.1257583  0.19904851]\n","Output of first sample: 3.2809093998783325\n","Stochastic Gradient Descent for Learning Rate: 0.007 produces the resulting Weight: [0.03424566 0.14754515 0.16636276]\n","Output of first sample: 3.1733247838837446\n","Stochastic Gradient Descent for Learning Rate: 0.008 produces the resulting Weight: [0.05024847 0.15178925 0.16582951]\n","Output of first sample: 3.226436066301784\n","Stochastic Gradient Descent for Learning Rate: 0.009000000000000001 produces the resulting Weight: [0.05171213 0.14488462 0.23968351]\n","Output of first sample: 3.8973933906405636\n","Stochastic Gradient Descent for Learning Rate: 0.01 produces the resulting Weight: [0.04854891 0.17827117 0.28361171]\n","Output of first sample: 4.667377758026527\n","Stochastic Gradient Descent for Learning Rate: 0.011 produces the resulting Weight: [0.04569411 0.18575893 0.21892919]\n","Output of first sample: 4.092575318372148\n","Stochastic Gradient Descent for Learning Rate: 0.012 produces the resulting Weight: [0.04875697 0.12692373 0.27366591]\n","Output of first sample: 4.054653412144529\n","Stochastic Gradient Descent for Learning Rate: 0.013000000000000001 produces the resulting Weight: [0.06502551 0.21320818 0.20522366]\n","Output of first sample: 4.249343864128827\n","Stochastic Gradient Descent for Learning Rate: 0.014000000000000002 produces the resulting Weight: [0.06028727 0.19765512 0.28261604]\n","Output of first sample: 4.862998802885782\n","\n"," ####################################### Different Num Iterations ############################################################################## \n","\n","Gradient Descent for Num_Iterations: 10 produces the resulting Weight: [0.08308768 0.2343672  0.26379581]\n","Output of first sample: 5.064717861811059\n","Gradient Descent for Num_Iterations: 11 produces the resulting Weight: [0.08557345 0.2356789  0.2632736 ]\n","Output of first sample: 5.0750984498077685\n","Gradient Descent for Num_Iterations: 12 produces the resulting Weight: [0.08783587 0.23618977 0.26196468]\n","Output of first sample: 5.069380323293929\n","Gradient Descent for Num_Iterations: 13 produces the resulting Weight: [0.09014629 0.23692135 0.26121783]\n","Output of first sample: 5.071538065236088\n","Gradient Descent for Num_Iterations: 14 produces the resulting Weight: [0.09236512 0.23737265 0.26031588]\n","Output of first sample: 5.06925033983765\n","Gradient Descent for Num_Iterations: 15 produces the resulting Weight: [0.09456525 0.23782594 0.25961223]\n","Output of first sample: 5.068947034442795\n","Gradient Descent for Num_Iterations: 16 produces the resulting Weight: [0.09670981 0.23815545 0.25890927]\n","Output of first sample: 5.067357105321985\n","Gradient Descent for Num_Iterations: 17 produces the resulting Weight: [0.09881874 0.23844356 0.25829622]\n","Output of first sample: 5.0662164833532355\n","Gradient Descent for Num_Iterations: 18 produces the resulting Weight: [0.10088264 0.23866265 0.25771575]\n","Output of first sample: 5.064666664187287\n","Gradient Descent for Num_Iterations: 19 produces the resulting Weight: [0.10290734 0.23883993 0.25718781]\n","Output of first sample: 5.063184700870739\n","\n","\n","Mini-Batch Gradient Descent for Num_Iterations: 10 produces the resulting Weight: [0.08942297 0.23010172 0.23712781]\n","Output of first sample: 4.761718353081214\n","Mini-Batch Gradient Descent for Num_Iterations: 11 produces the resulting Weight: [0.09299103 0.23113971 0.23571165]\n","Output of first sample: 4.761504713494613\n","Mini-Batch Gradient Descent for Num_Iterations: 12 produces the resulting Weight: [0.09648593 0.23199799 0.23443315]\n","Output of first sample: 4.760797353689692\n","Mini-Batch Gradient Descent for Num_Iterations: 13 produces the resulting Weight: [0.09990847 0.23270309 0.23327396]\n","Output of first sample: 4.759678931253856\n","Mini-Batch Gradient Descent for Num_Iterations: 14 produces the resulting Weight: [0.10325954 0.23327771 0.23221833]\n","Output of first sample: 4.758219944676057\n","Mini-Batch Gradient Descent for Num_Iterations: 15 produces the resulting Weight: [0.10654012 0.23374127 0.23125276]\n","Output of first sample: 4.756480494304377\n","Mini-Batch Gradient Descent for Num_Iterations: 16 produces the resulting Weight: [0.10975126 0.2341104  0.23036566]\n","Output of first sample: 4.7545117852173435\n","Mini-Batch Gradient Descent for Num_Iterations: 17 produces the resulting Weight: [0.11289405 0.23439928 0.22954706]\n","Output of first sample: 4.752357413034838\n","Mini-Batch Gradient Descent for Num_Iterations: 18 produces the resulting Weight: [0.11596964 0.23462007 0.22878842]\n","Output of first sample: 4.750054464454569\n","Mini-Batch Gradient Descent for Num_Iterations: 19 produces the resulting Weight: [0.11897917 0.23478315 0.22808238]\n","Output of first sample: 4.747634459258988\n","\n","\n","Stochastic Gradient Descent for Num_Iterations: 10 produces the resulting Weight: [0.05676124 0.1282228  0.24720231]\n","Output of first sample: 3.811012291600952\n","Stochastic Gradient Descent for Num_Iterations: 11 produces the resulting Weight: [0.04053948 0.21297297 0.25180462]\n","Output of first sample: 4.68831538873591\n","Stochastic Gradient Descent for Num_Iterations: 12 produces the resulting Weight: [0.05808701 0.21573639 0.26098788]\n","Output of first sample: 4.825329711005814\n","Stochastic Gradient Descent for Num_Iterations: 13 produces the resulting Weight: [0.04891616 0.16633247 0.24984129]\n","Output of first sample: 4.210653811732234\n","Stochastic Gradient Descent for Num_Iterations: 14 produces the resulting Weight: [0.0500321  0.18758446 0.26501576]\n","Output of first sample: 4.576034261577347\n","Stochastic Gradient Descent for Num_Iterations: 15 produces the resulting Weight: [0.06440621 0.20964368 0.22964981]\n","Output of first sample: 4.457341124530589\n","Stochastic Gradient Descent for Num_Iterations: 16 produces the resulting Weight: [0.05215215 0.17521784 0.29788966]\n","Output of first sample: 4.7832270644117845\n","Stochastic Gradient Descent for Num_Iterations: 17 produces the resulting Weight: [0.06392116 0.23837372 0.24032268]\n","Output of first sample: 4.850885173888521\n","Stochastic Gradient Descent for Num_Iterations: 18 produces the resulting Weight: [0.05378307 0.17906659 0.30514296]\n","Output of first sample: 4.895878481963143\n","Stochastic Gradient Descent for Num_Iterations: 19 produces the resulting Weight: [0.0553947  0.22218219 0.21329634]\n","Output of first sample: 4.4101800598804175\n"]}],"source":["import numpy as np\n","\n","# X: Features tensor, y: target tensor, learning_rate: step size, num_iterations: number of iterations\n","# Batch gradient descent\n","def gradient_descent(X, y, learning_rate, num_iterations):\n","  num_samples, num_features = X.shape\n","  # Init weight params as 0\n","  theta = np.zeros(num_features)\n","  for _ in range(num_iterations):\n","    # Dot X and Parameters to calculate Y_output. Take difference of Y_output and y as loss. Dot X Transpose and loss, divide by num_samples\n","    gradient = np.dot(X.T, (np.dot(X, theta) - y)) / num_samples\n","    # Update gradients in opposite direction\n","    theta -= learning_rate * gradient\n","  return theta\n","\n","# X: Features tensor, y: target tensor, learning_rate: step size, num_iterations: number of iterations\n","def mini_batch_gradient_descent(X, y, learning_rate, num_iterations, b):\n","  num_samples, num_features = X.shape\n","  theta = np.zeros(num_features)\n","  for _ in range(num_iterations):\n","    for i in range(0, len(y), b):\n","      X_i = X[i:i+b]\n","      y_i = y[i:i+b]\n","      gradient = np.dot(X_i.T, (np.dot(X_i, theta) - y_i)) / num_samples\n","      theta -= learning_rate * gradient\n","  return theta\n","\n","# X: Features tensor, y: target tensor, learning_rate: step size, num_iterations: number of iterations\n","def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n","  num_samples, num_features = X.shape\n","  theta = np.zeros(num_features)\n","  for _ in range(num_iterations):\n","    randIndex = np.random.randint(0, num_samples)\n","    X_i = X[randIndex, :]\n","    y_i = y[randIndex]\n","    gradient = np.dot(X_i.T, (np.dot(X_i, theta) - y_i)) / num_samples\n","    theta -= learning_rate * gradient\n","  return theta\n","\n","# X = features tensor of format: (Shelf_length, quantity in stock, import distance)\n","# y = label of sample, given as scalar value of price of fruit\n","sampleX = np.array([ [1,10,10],\n","                     [2,7,10],\n","                     [3,3,10],\n","                     [4,2,8],\n","                     [1,9,8],\n","                     [1,12,8],\n","                     [2,7,21],\n","                     [3,10,5],\n","                     [2,6,5],\n","                     [3,6,5] ])\n","fruitPrices = np.array([5, 4, 6, 3, 5.8, 5, 6, 3.5, 3.5, 2])\n","\n","gd_weights = gradient_descent(sampleX, fruitPrices, 0.01, 10)\n","mini_batch_gd_weights = mini_batch_gradient_descent(sampleX, fruitPrices, 0.01, 10, 2)\n","stochastic_gd_weights = stochastic_gradient_descent(sampleX, fruitPrices, 0.01, 10)\n","\n","print(gd_weights)\n","print(np.dot(sampleX[0], gd_weights))\n","\n","print(mini_batch_gd_weights)\n","print(np.dot(sampleX[0], mini_batch_gd_weights))\n","\n","print(stochastic_gd_weights)\n","print(np.dot(sampleX[0], stochastic_gd_weights))\n","\n","print(\"\\n ####################################### Different Learning Rates ############################################################################## \\n\")\n","\n","# loss = []\n","# plot (num_iterations, loss)\n","# loss = diff(y, np.dot(sampleX[0], gd_weights))\n","for j in range(0, 10):\n","  j_j = 0.005 + 0.001*j\n","  gd_weights = gradient_descent(sampleX, fruitPrices, j_j, 10)\n","  print(f\"Gradient Descent for Learning Rate: {j_j} produces the resulting Weight: {gd_weights}\")\n","  print(f\"Output of first sample: {np.dot(sampleX[0], gd_weights)}\")\n","\n","# plt.plot(num_iterations, loss, \"bo\", label=\"Training loss\")\n","# plt.plot(num_iterations, val_loss, \"b\", label=\"Validation loss\")\n","# plt.title(\"Training and validation loss\")\n","# plt.xlabel(\"Epochs\")\n","# plt.ylabel(\"Loss\")\n","# plt.legend()\n","# plt.show()\n","\n","print(\"\\n\")\n","\n","for j in range(0, 10):\n","  j_j = 0.005 + 0.001*j\n","  mini_batch_gd_weights = mini_batch_gradient_descent(sampleX, fruitPrices, j_j, 10, 2)\n","  print(f\"Mini_Batch Gradient Descent for Learning Rate: {j_j} produces the resulting Weight: {mini_batch_gd_weights}\")\n","  print(f\"Output of first sample: {np.dot(sampleX[0], mini_batch_gd_weights)}\")\n","\n","print(\"\\n\")\n","\n","for j in range(0, 10):\n","  j_j = 0.005 + 0.001*j\n","  stochastic_gd_weights = stochastic_gradient_descent(sampleX, fruitPrices, j_j, 10)\n","  print(f\"Stochastic Gradient Descent for Learning Rate: {j_j} produces the resulting Weight: {stochastic_gd_weights}\")\n","  print(f\"Output of first sample: {np.dot(sampleX[0], stochastic_gd_weights)}\")\n","\n","\n","print(\"\\n ####################################### Different Num Iterations ############################################################################## \\n\")\n","\n","for j in range(10, 20):\n","  gd_weights = gradient_descent(sampleX, fruitPrices, 0.01, j)\n","  print(f\"Gradient Descent for Num_Iterations: {j} produces the resulting Weight: {gd_weights}\")\n","  print(f\"Output of first sample: {np.dot(sampleX[0], gd_weights)}\")\n","\n","print(\"\\n\")\n","\n","for j in range(10, 20):\n","  mini_batch_gd_weights = mini_batch_gradient_descent(sampleX, fruitPrices, 0.01, j, 2)\n","  print(f\"Mini-Batch Gradient Descent for Num_Iterations: {j} produces the resulting Weight: {mini_batch_gd_weights}\")\n","  print(f\"Output of first sample: {np.dot(sampleX[0], mini_batch_gd_weights)}\")\n","\n","print(\"\\n\")\n","\n","for j in range(10, 20):\n","  stochastic_gd_weights = stochastic_gradient_descent(sampleX, fruitPrices, 0.01, j)\n","  print(f\"Stochastic Gradient Descent for Num_Iterations: {j} produces the resulting Weight: {stochastic_gd_weights}\")\n","  print(f\"Output of first sample: {np.dot(sampleX[0], stochastic_gd_weights)}\")\n","\n","\n","\n","\n"]}]}